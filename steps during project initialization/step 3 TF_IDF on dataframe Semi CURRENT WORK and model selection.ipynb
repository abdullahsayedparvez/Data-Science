{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d113171e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: You must give at least one requirement to install (see \"pip help install\")\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ff7ca2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abdul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import re\n",
    "import time\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime \n",
    "import json\n",
    "import requests\n",
    "from pymongo.errors import BulkWriteError\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import joblib \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_curve\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from joblib import dump, load\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d1f8ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = pymongo.MongoClient(\"mongodb://localhost:27017\")\n",
    "# db = client['Final_Year_Project']\n",
    "# collection = db['stock_twits_dataset']\n",
    "# records = collection.find({})   # Getting all the dociments from mongodb\n",
    "# # print(type(records))  # checking the type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b1a12d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_records = [record for record in records]\n",
    "# list_records = list(records) \n",
    "# df = pd.DataFrame(list_records)\n",
    "# # df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3e515e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in records:\n",
    "#     p = 1\n",
    "#     print(i)\n",
    "#     p = p + 1\n",
    "#     if p == 10 :\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d0258df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DATA FOR PREDICTION (it is yesterday data of the current data)\n",
    "stock_name = 'EASEMYTRIP.NSE'\n",
    "header = {'Accept' : 'application/json',\n",
    "  'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36',\n",
    "    'Accept-Encoding':'gzip, deflate, br',\n",
    "    'Accept-Language' :  'en-IN,en-GB;q=0.9,en-US;q=0.8,en;q=0.7,hi;q=0.6',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Sec-Fetch-Site':'same-site',\n",
    "    'Sec-Fetch-Mode':'cors',\n",
    "    'Sec-Fetch-Dest':'empty'}\n",
    "\n",
    "\n",
    "              # this name will come from flask frontend\n",
    "main_data = []                    #  All data will store here\n",
    "api_next_list = []\n",
    "\n",
    "\n",
    "time = 24                             # Change according to number of days\n",
    "current_time = datetime.now()                 # It show current time\n",
    "time = current_time - timedelta(hours=time)   # it will go 24 hours back and give you a specific time and time\n",
    "# time = 24\n",
    "# date_of_testing ='12/15/23'\n",
    "# date_of_testing_datetime = datetime.strptime(date_of_testing, '%m/%d/%y')\n",
    "# time = date_of_testing_datetime - timedelta(hours=time)\n",
    "\n",
    "#Initializing the request and converts it into json\n",
    "url = f'https://api.stocktwits.com/api/2/streams/symbol/{stock_name}.json?filter=top&limit=22'  \n",
    "request_success = requests.get(url,headers=header)   \n",
    "json_data_dict = request_success.json()\n",
    "\n",
    "\n",
    "#Taking all the usefull data from the json data\n",
    "usefull_data = (json_data_dict['messages']) \n",
    "\n",
    "#picking the next api address for further scrapping\n",
    "next_api_first = json_data_dict['cursor']['max']\n",
    "next_api_first = str(next_api_first)\n",
    "next_api_first = '&max='+next_api_first\n",
    "\n",
    "\n",
    "\n",
    "# Retrieving the last date from twit to break the first loop\n",
    "last_twit_date_1 = (json_data_dict['messages'][-1])\n",
    "last_twit_date_1 = last_twit_date_1['created_at']\n",
    "last_twit_date_1 = re.sub(pattern = \"[T-Z]+\",repl = ' ',string=last_twit_date_1) # eliminating alphabets\n",
    "last_twit_date_1 = datetime.strptime(last_twit_date_1, \"%Y-%m-%d  %H:%M:%S \")  # changing datatype\n",
    "\n",
    "\n",
    "\n",
    "#Looping in the usefull_data\n",
    "for data in usefull_data:\n",
    "    # Getting the twit date from  usefull_data and cleaning is done\n",
    "    twit_date = data['created_at']\n",
    "    twit_date = re.sub(pattern = \"[T-Z]+\",repl = ' ',string=twit_date)\n",
    "    twit_date = datetime.strptime(twit_date, \"%Y-%m-%d  %H:%M:%S \")   \n",
    "\n",
    "    if twit_date > time:\n",
    "        twit = data['body']\n",
    "        signal = data['entities']['sentiment']\n",
    "        if signal == None:\n",
    "            signal = 'Null'\n",
    "        else :\n",
    "            signal = signal['basic']\n",
    "        stock_twits_data = {'stock_name':stock_name ,'Date':twit_date,'Twits': twit,'Signal': signal,'Cleaning_status':'pending','Cleaning_status_lemmatization':'pending'}\n",
    "        main_data.append(stock_twits_data)\n",
    "    if twit_date < time:\n",
    "        if len(main_data) == 0:\n",
    "            print('No twits found')\n",
    "        else:\n",
    "            print('twits are there')\n",
    "        break\n",
    "    try:\n",
    "        if time < last_twit_date_1.strptime('%H:%M:%S'):\n",
    "            break\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#########################################################################################################################################    \n",
    "while (True):\n",
    "    loop_break = []\n",
    "    url =  f'https://api.stocktwits.com/api/2/streams/symbol/{stock_name}.json?filter=top&limit=22{next_api_first}'\n",
    "    request_success = requests.get(url,headers=header)   \n",
    "    json_data_dict = request_success.json() \n",
    "    next_api_first = json_data_dict['cursor']['max']\n",
    "    next_api_first = str(next_api_first)\n",
    "    next_api_first = '&max='+next_api_first\n",
    "    usefull_data = (json_data_dict['messages'])\n",
    "    for data in usefull_data:\n",
    "        twit_date = data['created_at']\n",
    "        twit_date = re.sub(pattern = \"[T-Z]+\",repl = ' ',string=twit_date)\n",
    "        twit_date = datetime.strptime(twit_date, \"%Y-%m-%d  %H:%M:%S \")   \n",
    "        if twit_date > time:\n",
    "            twit = data['body']\n",
    "            signal = data['entities']['sentiment']\n",
    "            if signal == None:\n",
    "                signal = 'Null'\n",
    "            else :\n",
    "                signal = signal['basic']\n",
    "            stock_twits_data = {'stock_name':stock_name ,'Date':twit_date,'Twits': twit,'Signal':signal,'Cleaning_status':'pending','Cleaning_status_lemmatization':'pending'}\n",
    "            main_data.append(stock_twits_data)\n",
    "\n",
    "\n",
    "\n",
    "    if twit_date < time:\n",
    "        print('while loop break')\n",
    "        break\n",
    "print(main_data)\n",
    "if not main_data:    \n",
    "    print('No data is found')\n",
    "else :\n",
    "    df = pd.DataFrame(main_data)\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "    ##Removing Links from dataframe\n",
    "    print('---------------------------------Links------------------------------------')\n",
    "    df['Twits'] = df['Twits'].replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)\n",
    "\n",
    "\n",
    "    ## Lowercase the whole twits column\n",
    "    print('---------------------------------lowercase------------------------------------')\n",
    "    df['Twits'] = df['Twits'].apply(lambda x: x.lower())\n",
    "\n",
    "\n",
    "    ## Eliminating the pumctuation \n",
    "    print('----------------------------eliminate punctuation-------------------------------')\n",
    "    df['Twits'] = df['Twits'].str.replace('[^\\w\\s]',' ',regex=True)\n",
    "\n",
    "\n",
    "    ##Eliminating stopwords \n",
    "    def remove_stopwords(x):\n",
    "        return \" \".join([word for word in str(x).split() if word not in stop_words])\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    print('----------------------------removing stopwords-------------------------------')\n",
    "    df['Twits'] = df['Twits'].apply(lambda x : remove_stopwords(x))\n",
    "\n",
    "\n",
    "    ##Eliminating Emoji from dataframe\n",
    "    print('----------------------------removing emojis-------------------------------')\n",
    "    df['Twits'] = df['Twits'].apply(lambda s: emoji.replace_emoji(s, ''))\n",
    "\n",
    "    # Stemming on dataframe\n",
    "    print('----------------------------stemming is done-------------------------------')\n",
    "    stemmer = PorterStemmer()\n",
    "    df['Twits'] = df['Twits'].str.split()\n",
    "    df['Twits'] = df['Twits'].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word.\n",
    "    df['Twits'] = df['Twits'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "    # Removing number str from column twits\n",
    "    print('----------------------------removing number is done-------------------------------')\n",
    "    df['Twits'] = df['Twits'].replace('\\d+', '', regex=True)\n",
    "\n",
    "    # Removing the words which contain only 1 letter\n",
    "    print('----------------------------removing 1 letter word is done-------------------------------')\n",
    "    df['Twits'] = df['Twits'].str.split()\n",
    "    for text in df['Twits']:\n",
    "        for word in text:\n",
    "            if len(word) == 1:\n",
    "                text.remove(word)\n",
    "    df['Twits'] = df['Twits'].apply(lambda x: \" \".join(x))\n",
    "    df_yesterday = df.drop(['stock_name','Date','Cleaning_status','Cleaning_status_lemmatization','Signal'], axis=1)\n",
    "    df_yesterday = df_yesterday.drop_duplicates()\n",
    "    print(df_yesterday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c6b9ea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_yesterday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58160fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9cab43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5eac99d",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "38cc80c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c01b2bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['_id', 'index','stock_name','Date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff4c3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find duplicate value \n",
    "# duplicate = df[df.duplicated()]\n",
    "\n",
    "#drop all duplicate rows from dataframe (df)\n",
    "df = df.drop_duplicates()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0c88bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df['twits']=='tatasteel nse'].drop_duplicates()  # see all the tatasteel.nse rows from here\n",
    "df = df.drop([0, 8 ,492])   # it will remove the rows that contains tatasteel.nse \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c893cb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> take all the null signal value for testing value      ##### Testing data named----> df_testing\n",
    "\n",
    "df_null = df[df['signal'] == 'Null']\n",
    "df_testing = df_null\n",
    "df_testing_x_test = df_testing['twits']\n",
    "df_testing_x_test = df_testing_x_test.astype('unicode')\n",
    "df_testing_y_test = df_testing['signal']\n",
    "# df_testing['twits'] = df_testing['twits'].astype('unicode')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe8857a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d25fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fc24cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e474fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --> take all the bearish signals value      ##### training data\n",
    "\n",
    "df_bearish = df[df['signal'] == 'Bearish']\n",
    "df_bearish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e3dbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --> take all the bullish signals value      ##### training data\n",
    "\n",
    "df_bullish = df[df['signal'] == 'Bullish']\n",
    "df_bullish = df_bullish.reset_index(drop=True)   # reset index to count the numbers properly\n",
    "df_bullish = df_bullish.iloc[0:665007]           # iloc is used to equalize the Bearish and Bullish counts\n",
    "df_bullish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39078679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --> merging two dataframe for training dataset\n",
    "\n",
    "dataframes = [df_bullish,df_bearish]\n",
    "df = pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738beda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23114d43",
   "metadata": {},
   "source": [
    "# The Above Implemantation is different and the below implementation is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b34d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14db5722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> take all the null signal value for testing value      ##### Testing data named----> df_testing\n",
    "\n",
    "\n",
    "# df_null = df[df['signal'] == 'Null']\n",
    "# df_testing = df_null\n",
    "# df_testing  # How many null values are there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071d8952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -->Only get Bullisg and Bearish value only eliminating null value    ##### Training data named-----> df\n",
    "\n",
    "\n",
    "# df.drop(df[(df['signal']=='Null')].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac5e938",
   "metadata": {},
   "source": [
    "# The Below code is common for both methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6028d5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new column for target value named == 'signal for machine'\n",
    "\n",
    "df['signal for machine'] = df['signal'].map({'Bullish': 1,'Bearish': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5e8f4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0725fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the target value the number of count of bullish and bearish (1,0)\n",
    "df['signal for machine'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efc5a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> First convert the Object dtype to unicode and then go to train test split\n",
    "\n",
    "\n",
    "df['twits'] = df['twits'].astype('unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08429b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['twits'], df['signal for machine'], test_size=0.1,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e769553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aedce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519eb880",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf69900",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb8b7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF Implementation now Training data and testing data\n",
    "vect = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef895c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the tfidf vectorizer and saving it as a file\n",
    "X_train_vectorized = vect.fit(X_train)\n",
    "print(type(X_train_vectorized))\n",
    "joblib.dump(X_train_vectorized, 'tfidf_vectorizer_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544c9670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a tfidf vectorizer and transforming X_train and X_test\n",
    "\n",
    "loaded_tfidf_vectorizer = joblib.load('tfidf_vectorizer_2.pkl')\n",
    "X_train_vectorized = loaded_tfidf_vectorizer.transform(X_train)\n",
    "print(type(X_train_vectorized))\n",
    "X_test_vectorized  = loaded_tfidf_vectorizer.transform(X_test)\n",
    "print(type(X_test_vectorized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7534433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758c8fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This converts the words into decimals so the model can understand    testing data\n",
    "# X_test_vectorized = vect.transform(X_test)\n",
    "# pd.DataFrame(X_test_vectorized)\n",
    "# print(\"Shape of X_test_tfidf:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955bb84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This converts the words into decimals so the model can understand    dataset wants to predicts (Null values dataset)\n",
    "loaded_tfidf_vectorizer = joblib.load('tfidf_vectorizer_2.pkl')\n",
    "x_test_prediction = loaded_tfidf_vectorizer.transform(df_testing_x_test)\n",
    "print(\"Shape of X_test_null_value_data:\", x_test_prediction.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d17b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yesterday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379d32c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This converts the words into decimals so the model can understand    dataset wants to predict (yesterday dataset)\n",
    "# df_yesterday = df_yesterday['Twits'].astype('unicode')\n",
    "# tfidf_vectorizer_prediction_yesterday = TfidfVectorizer(max_features=1000)\n",
    "# x_test_prediction_yesterday = tfidf_vectorizer_prediction_yesterday.fit_transform(df_yesterday)\n",
    "# print(\"Shape of yesterday_dataset:\", x_test_prediction_yesterday.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b195bf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(x_test_prediction_yesterday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667c1e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(x_test_prediction_yesterday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fe7882",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_yesterday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cf8640",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Making prediction on a single twits Merging all the rows of dataframe   IMPO\n",
    "# Merging all therows of dataframe in a list  \n",
    "pd.DataFrame(df_yesterday)\n",
    "df_yesterday_combine         = df_yesterday                                            # duplicating the df_yesterday\n",
    "twits_prediction_lists       = df_yesterday_combine['Twits'].tolist()                                   # Converting it into list\n",
    "twits_prediction_lists_merge = \" \".join(twits_prediction_lists)                        # joining each element of a list into a single element\n",
    "dict_yesterday_twits_merge   = {'Twits':twits_prediction_lists_merge}                  # Initializing of creating dataframe  \n",
    "df_yesterday_twits_merge     = pd.DataFrame(dict_yesterday_twits_merge,index=['Index'])# making dataframe\n",
    "df_yesterday_twits_merge_unicode  = df_yesterday_twits_merge['Twits'].astype('unicode')# Converting it into unicode\n",
    "loaded_tfidf_vectorizer = load('tfidf_vectorizer.pkl')\n",
    "x_test_prediction_yesterday_merge = loaded_tfidf_vectorizer.transform(df_yesterday_twits_merge_unicode) \n",
    "print(\"Shape of yesterday_dataset:\", x_test_prediction_yesterday_merge.shape)          #Checking the shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0205203c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "753ec393",
   "metadata": {},
   "source": [
    "# Linear Regression   ( 78 Accuracy )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c5ce31",
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating Model using Logistic regression   X_train_vectorized twits(decimals tfidf)     y_train contain (1 and 0)  target data\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_vectorized,y_train)  # actucal training of data will we here\n",
    "\n",
    "\n",
    "\n",
    "## Can give any test data but first convert it into object into unicode     X_test twits (decimals tfidf)    # we can give any dataset here\n",
    "## converting into decimals with tfidf\n",
    "# predictions = model.predict(vect.transform(df_testing_x_test))\n",
    "predictions = model.predict(X_test_vectorized)     # Give here df_testing\n",
    "\n",
    "\n",
    "# It is in Numbers means 0 and 1\n",
    "print(predictions)  # the preduicted values is in 1 == Bullish and 0 == Bearish\n",
    "\n",
    "\n",
    "# How to convert numbers into str \n",
    "predictions_in_str = []\n",
    "print(predictions_in_str)\n",
    "for i in predictions:\n",
    "    i = round(i) # getting approximate value by using round function \n",
    "    i = str(i)  # First we are converting it into string \n",
    "    predictions_in_str.append(i)  # appending in a empty list\n",
    "    \n",
    "\n",
    "predictions_in_str = list(map(lambda x: x.replace('1', 'Bullish'), predictions_in_str))   # Converting 1 into Bullish \n",
    "predictions_in_str = list(map(lambda x: x.replace('0', 'Bearish'), predictions_in_str))   # Converting 0 into Bearish\n",
    "\n",
    "\n",
    "# # if you want to see the changes in the predictions_in_str \n",
    "# for i in predictions_in_str:\n",
    "# #     print(i)\n",
    "#     z = 20\n",
    "#     q = 1\n",
    "#     q = q +1\n",
    "#     if q == 20:\n",
    "#         break\n",
    "        \n",
    "# compute AUC value--> Area Under Curve\n",
    "# print(\"AUC:\",roc_auc_score(y_test,predictions))\n",
    "\n",
    "\n",
    "# accuracy value of linear regression\n",
    "predictions_list_converts_again = []\n",
    "for i in predictions:\n",
    "    i = round(i)\n",
    "    predictions_list_converts_again.append(i)\n",
    "print(accuracy_score(y_test,predictions_list_converts_again))\n",
    "# model_train_accuracy = accuracy_score(y_train,y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df831fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here we Combine the X_test and the predicted value\n",
    "print(len(predictions_in_str))\n",
    "df_result_final = pd.DataFrame(X_test)\n",
    "df_result_final['Predicted value final'] = predictions_in_str\n",
    "df_result_final = df_result_final.sort_index(axis=0)  # sort in ascending order\n",
    "\n",
    "df_result_final.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c681cf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model and loading it\n",
    "dump(model, 'linear_regression_final.joblib')\n",
    "# loaded_model = load('linear_regression.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21741bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on NULL dataset\n",
    "loaded_model = load('linear_regression_final.joblib')\n",
    "predictions = loaded_model.predict(x_test_prediction)  # now see the yesterday dataseton this line\n",
    "print(predictions)\n",
    "pd.DataFrame(df_testing_x_test)\n",
    "# df_testing_x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff852229",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making prediction on a single merge twit yesterday data\n",
    "loaded_model = load('linear_regression_final.joblib')\n",
    "predictions = loaded_model.predict(x_test_prediction_yesterday_merge)\n",
    "print(f'The predicted value is {predictions}')\n",
    "if predictions > 0.5 :\n",
    "    print('Bullish')\n",
    "else:\n",
    "    print('Bearish')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4158c787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on yesterday dataset\n",
    "loaded_model = load('linear_regression.joblib')\n",
    "predictions = loaded_model.predict(x_test_prediction_yesterday)  # now see the yesterday dataseton this line\n",
    "results_final = []\n",
    "for signal in predictions:\n",
    "    if signal >= 0.5:\n",
    "        results_final.append(\"Bullish\")\n",
    "    else:\n",
    "        results_final.append(\"Bearish\") \n",
    "print(len(predictions))\n",
    "\n",
    "# Combining the twits and predictions of yesterday data tranforming it into dataframe\n",
    "df_yesterday_prediction = pd.DataFrame(df_yesterday)                     # creating a dataframe of yesterday twits\n",
    "df_yesterday_prediction['Yesterday_results_prediction'] = results_final  # adding the prediction value in new column \n",
    "df_yesterday_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75947c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final decision of Bullisg and Bearish \n",
    "bullish = []\n",
    "bearish = []\n",
    "for i in results_final:\n",
    "    if i == 'Bullish':\n",
    "        bullish.append(i)\n",
    "    else:\n",
    "        bearish.append(i)\n",
    "if len(bullish) > len(bearish):\n",
    "    print('Stock is Bullish')\n",
    "else:\n",
    "    print('Stock is Bearish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67140816",
   "metadata": {},
   "outputs": [],
   "source": [
    "twits_prediction_lists_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffa332c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "point_five_greater_bullish = []\n",
    "point_five_lesser_bearish  = []\n",
    "for pred in predictions:\n",
    "    if pred > 0.5 :\n",
    "        point_five_greater_bullish.append(pred)\n",
    "    else:\n",
    "        point_five_lesser_bearish.append(pred)\n",
    "bullish_list_numbers = sum(point_five_greater_bullish)\n",
    "bearish_list_numbers = sum(point_five_lesser_bearish)\n",
    "print(bullish_list_numbers)\n",
    "print(bearish_list_numbers)\n",
    "if bullish_list_numbers > bearish_list_numbers:\n",
    "    print('Stock is Bullish')\n",
    "else :\n",
    "    print('Stock is Bearish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf2b785",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620b2450",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bullish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2d995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bearish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eb5059",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1859b9",
   "metadata": {},
   "source": [
    "# Logistic regression   (79 Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aa16f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating Model using Logistic regression   X_train_vectorized twits(decimals tfidf)     y_train contain (1 and 0)  target data\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized,y_train)\n",
    "\n",
    "\n",
    "\n",
    "## Can give any test data but first convert it into object into unicode     X_test twits (decimals tfidf)    # we can give any dataset here\n",
    "## converting into decimals with ifidf\n",
    "predictions = model.predict(X_test_vectorized)    # Give here df_testing\n",
    "\n",
    "\n",
    "# It is in Numbers means 0 and 1\n",
    "print(predictions)  # the preduicted values is in 1 == Bullish and 0 == Bearish\n",
    "\n",
    "\n",
    "# How to convert numbers into str \n",
    "predictions_in_str = []\n",
    "for i in predictions:\n",
    "    i = str(i)  # First we are converting it into string \n",
    "    predictions_in_str.append(i)  # appending in a empty list\n",
    "predictions_in_str = list(map(lambda x: x.replace('1', 'Bullish'), predictions_in_str))   # Converting 1 into Bullish \n",
    "predictions_in_str = list(map(lambda x: x.replace('0', 'Bearish'), predictions_in_str))   # Converting 0 into Bearish\n",
    "\n",
    "\n",
    "# # if you want to see the changes in the predictions_in_str \n",
    "# for i in predictions_in_str:\n",
    "# #     print(i)\n",
    "#     z = 20\n",
    "#     q = 1\n",
    "#     q = q +1\n",
    "#     if q == 20:\n",
    "#         break\n",
    "        \n",
    "# compute AUC value--> Accuracy\n",
    "predictions_list_converts_again = []\n",
    "for i in predictions:\n",
    "    i = round(i)\n",
    "    predictions_list_converts_again.append(i)\n",
    "print(accuracy_score(y_test,predictions_list_converts_again))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e586610",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here we Combine the X_test and the predicted value\n",
    "print(len(predictions_in_str))\n",
    "df_result_final = pd.DataFrame(X_test)\n",
    "df_result_final['Predicted value final'] = predictions_in_str\n",
    "df_result_final = df_result_final.sort_index(axis=0)  # sort in ascending order\n",
    "\n",
    "df_result_final.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c1dc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model and loading it\n",
    "dump(model, 'logistic_regression_final.joblib')\n",
    "# loaded_model = load('linear_regression.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd2ac3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on NULL dataset\n",
    "loaded_model = load('linear_regression_final.joblib')\n",
    "predictions = loaded_model.predict(x_test_prediction)  # now see the yesterday dataseton this line\n",
    "print(predictions)\n",
    "pd.DataFrame(df_testing_x_test)\n",
    "# df_testing_x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9e1c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making prediction on a single merge twit yesterday data\n",
    "loaded_model = load('logistic_regression_final.joblib')\n",
    "predictions = loaded_model.predict(x_test_prediction_yesterday_merge)\n",
    "print(f'The predicted value is {predictions}')\n",
    "if predictions > 0.5 :\n",
    "    print('Bullish')\n",
    "else:\n",
    "    print('Bearish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003ba9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on yesterday dataset\n",
    "loaded_model = load('logistic_regression.joblib')\n",
    "predictions = loaded_model.predict(x_test_prediction_yesterday)  # now see the yesterday dataseton this line\n",
    "results_final = []\n",
    "for signal in predictions:\n",
    "    if signal >= 0.5:\n",
    "        results_final.append(\"Bullish\")\n",
    "    else:\n",
    "        results_final.append(\"Bearish\") \n",
    "print(len(predictions))\n",
    "\n",
    "# Combining the twits and predictions of yesterday data tranforming it into dataframe\n",
    "df_yesterday_prediction = pd.DataFrame(df_yesterday)                     # creating a dataframe of yesterday twits\n",
    "df_yesterday_prediction['Yesterday_results_prediction'] = results_final  # adding the prediction value in new column \n",
    "df_yesterday_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76745777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final decision of Bullisg and Bearish \n",
    "bullish = []\n",
    "bearish = []\n",
    "for i in results_final:\n",
    "    if i == 'Bullish':\n",
    "        bullish.append(i)\n",
    "    else:\n",
    "        bearish.append(i)\n",
    "if len(bullish) > len(bearish):\n",
    "    print('Stock is Bullish')\n",
    "else:\n",
    "    print('Stock is Bearish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b64afdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_five_greater_bullish = []\n",
    "point_five_lesser_bearish  = []\n",
    "for pred in predictions:\n",
    "    if pred > 0.5 :\n",
    "        point_five_greater_bullish.append(pred)\n",
    "    else:\n",
    "        point_five_lesser_bearish.append(pred)\n",
    "bullish_list_numbers = sum(point_five_greater_bullish)\n",
    "bearish_list_numbers = sum(point_five_lesser_bearish)\n",
    "print(bullish_list_numbers)\n",
    "print(bearish_list_numbers)\n",
    "if bullish_list_numbers > bearish_list_numbers:\n",
    "    print('Stock is Bullish')\n",
    "else :\n",
    "    print('Stock is Bearish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eee954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8421c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a31feff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67c5706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on yesterday dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb10e5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def GiveYourDataset(dataset):\n",
    "#     dataset = dataset.astype('unicode')   # converting dataset to unicodes\n",
    "#     print('step 01 done : converting dataset to unicodes ')\n",
    "    \n",
    "#     tfidf_vectorizer_prediction_ = TfidfVectorizer(max_features=10000)  # creating tfidf\n",
    "#     print('step 02 done : creating tfidf')\n",
    "    \n",
    "#     x_test_prediction = tfidf_vectorizer_prediction_.fit_transform(dataset) # fitting and transform tfidf\n",
    "#     print('step 03 done: fitting and transform tfidf')\n",
    "    \n",
    "#     shape_of_prediction = x_test_prediction.shape\n",
    "#     loaded_model = load('linear_regression.joblib')  # loading the model\n",
    "#     print('step 04 done : loading the model')\n",
    "    \n",
    "#     predictions_final = loaded_model.predict(x_test_prediction)  # making predictions\n",
    "#     print('step 05 done : Prediction done all set')\n",
    "    \n",
    "#     return predictions_final, shape_of_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a07baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GiveYourDataset(df_yesterday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d442b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # appending in a list the repected columns value i.e signal\n",
    "\n",
    "# actual_value = y_test.tolist()    # here we are converting y_test which is series into list\n",
    "\n",
    "# # How to convert numbers into str \n",
    "# actual_value_in_str = []\n",
    "# for i in actual_value:\n",
    "#     i = str(i)  # First we are converting it into string \n",
    "#     actual_value_in_str.append(i)  # appending in a empty list\n",
    "    \n",
    "\n",
    "# actual_value_in_str = list(map(lambda x: x.replace('1', 'Bullish'), actual_value_in_str))   # Converting 1 into Bullish \n",
    "# actual_value_in_str = list(map(lambda x: x.replace('0', 'Bearish'), actual_value_in_str))   # Converting 0 into Bearish\n",
    "\n",
    "\n",
    "# # Making one column from datframe into list\n",
    "\n",
    "# predicted_value = df_result['Predicted value'].to_list()\n",
    "\n",
    "# predicted_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197ba82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = 0\n",
    "# for i in predicted_value:\n",
    "#     p = p + 1\n",
    "#     print(i)\n",
    "#     if p > 10:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d442a715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(actual_value_in_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daf0db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(predicted_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce4bc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> entry that is having 'tatasteel nse' solv it later\n",
    "\n",
    "# df[df['twits']=='tatasteel nse']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fbe745",
   "metadata": {},
   "source": [
    "# Calculating Confusion matrix  --> When your data is unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36e6c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training dataset(Which is ground truth) and predicted dataset(Which dataset you predicted)\n",
    "# cm = confusion_matrix(actual_value_in_str,predicted_value)\n",
    "\n",
    "# For ideal confusion matrix it will show perfection\n",
    "# cm2= confusion_matrix(actual_value_in_str,actual_value_in_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa499a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cm2)   # this we want "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af80c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cm)  # We are gettinmg this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4bfafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import fbeta_score, make_scorer\n",
    "# ftwo_scorer = make_scorer(fbeta_score, beta=2, average='weighted')\n",
    "# scorer = sklearn.metrics.make_scorer(sklearn.metrics.f1_score, average = 'weighted')\n",
    "# gs_svc = GridSearchCV(estimator=svc_clf,param_grid=param_grid,scoring=scorer,cv=5)\n",
    "# scoring = {'accuracy': make_scorer(accuracy_score),\n",
    "#            'precision': make_scorer(precision_score, average = 'macro'),\n",
    "#            'recall': make_scorer(recall_score, average = 'macro'),\n",
    "#            'f1_macro': make_scorer(f1_score, average = 'macro'),\n",
    "#            'f1_weighted': make_scorer(f1_score, average = 'weighted')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4798761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculating precision\n",
    "\n",
    "# precision_score(actual_value_in_str,predicted_value, pos_label=\"Bullish\")  # pos_label --> which one is positive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1197ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating recall\n",
    "# recall_score(actual_value_in_str,predicted_value, pos_label=\"Bullish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b614d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating F1-score\n",
    "# f1_score(actual_value_in_str,predicted_value, pos_label=\"Bullish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab20f5d",
   "metadata": {},
   "source": [
    "# Redge and lasso - If overfitting (model working good with train data and working bad with testing data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fd2295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e27496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a715b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdf953b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee02de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision, recall, threshold = precision_recall_curve(actual_value_in_str,predicted_value, pos_label=\"Bullish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006044a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(['signal for machine'], axis=1)\n",
    "# df = df.set_index('twits',inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375d5db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa370e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff090be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_result = df_result.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5ad692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1740814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4319b1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2997f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cc6d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
